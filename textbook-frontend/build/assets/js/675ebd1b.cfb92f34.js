"use strict";(globalThis.webpackChunktextbook_frontend=globalThis.webpackChunktextbook_frontend||[]).push([[9607],{7537:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapter-11-capstone","title":"Chapter 11: Capstone Project - The Autonomous Humanoid","description":"Project Overview","source":"@site/docs/chapter-11-capstone.md","sourceDirName":".","slug":"/chapter-11-capstone","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter-11-capstone","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter-11-capstone.md","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"sidebar_position":12},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 10: Conversational Robotics with LLMs","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter-10-conversational"}}');var t=r(4848),o=r(8453);const s={sidebar_position:12},l="Chapter 11: Capstone Project - The Autonomous Humanoid",a={},c=[{value:"Project Overview",id:"project-overview",level:2},{value:"Project Scenario",id:"project-scenario",level:3},{value:"System Architecture",id:"system-architecture",level:2},{value:"Phase 1: Environment Setup",id:"phase-1-environment-setup",level:2},{value:"Create a Home Environment",id:"create-a-home-environment",level:3},{value:"Phase 2: Perception Pipeline",id:"phase-2-perception-pipeline",level:2},{value:"Object Detection with YOLO",id:"object-detection-with-yolo",level:3},{value:"Visual SLAM for Localization",id:"visual-slam-for-localization",level:3},{value:"Phase 3: Task Planning with LLM",id:"phase-3-task-planning-with-llm",level:2},{value:"Hierarchical Task Decomposition",id:"hierarchical-task-decomposition",level:3},{value:"Phase 4: Action Execution",id:"phase-4-action-execution",level:2},{value:"Navigation Action",id:"navigation-action",level:3},{value:"Manipulation Action (Simplified)",id:"manipulation-action-simplified",level:3},{value:"Phase 5: Integration",id:"phase-5-integration",level:2},{value:"Main Controller",id:"main-controller",level:3},{value:"Phase 6: Testing and Validation",id:"phase-6-testing-and-validation",level:2},{value:"Test Cases",id:"test-cases",level:3},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Submission Requirements",id:"submission-requirements",level:2},{value:"1. GitHub Repository",id:"1-github-repository",level:3},{value:"2. Demo Video (90 seconds max)",id:"2-demo-video-90-seconds-max",level:3},{value:"3. Documentation",id:"3-documentation",level:3},{value:"Evaluation Rubric",id:"evaluation-rubric",level:2},{value:"Bonus Challenges",id:"bonus-challenges",level:2},{value:"Challenge 1: Multi-Robot Coordination",id:"challenge-1-multi-robot-coordination",level:3},{value:"Challenge 2: Adaptive Behavior",id:"challenge-2-adaptive-behavior",level:3},{value:"Challenge 3: Real-World Deployment",id:"challenge-3-real-world-deployment",level:3},{value:"Final Thoughts",id:"final-thoughts",level:2},{value:"Submission",id:"submission",level:2},{value:"Congratulations!",id:"congratulations",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-11-capstone-project---the-autonomous-humanoid",children:"Chapter 11: Capstone Project - The Autonomous Humanoid"})}),"\n",(0,t.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Goal"}),": Build a simulated humanoid robot that demonstrates the full Physical AI stack:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception"}),": Visual SLAM, object detection"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reasoning"}),": Natural language understanding, task planning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action"}),": Navigation, manipulation, balance control"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Interaction"}),": Voice commands, conversational AI"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"project-scenario",children:"Project Scenario"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task"}),': "Robot, go to the kitchen, find the red cup, and bring it to me."']}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Required Capabilities"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Speech recognition (Whisper)"}),"\n",(0,t.jsx)(n.li,{children:"Task decomposition (GPT-4/Gemini)"}),"\n",(0,t.jsx)(n.li,{children:"Navigation (Nav2)"}),"\n",(0,t.jsx)(n.li,{children:"Object detection (YOLO/SAM)"}),"\n",(0,t.jsx)(n.li,{children:"Manipulation (inverse kinematics)"}),"\n",(0,t.jsx)(n.li,{children:"Human-robot interaction (TTS)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:"graph TD\r\n    A[User Voice Command] --\x3e B[Whisper STT]\r\n    B --\x3e C[LLM Task Planner]\r\n    C --\x3e D{Task Type}\r\n    D --\x3e|Navigate| E[Nav2 Stack]\r\n    D --\x3e|Manipulate| F[MoveIt2]\r\n    D --\x3e|Perceive| G[Computer Vision]\r\n    E --\x3e H[Robot Base Controller]\r\n    F --\x3e I[Arm Controller]\r\n    G --\x3e J[Object Detector]\r\n    J --\x3e F\r\n    H --\x3e K[Gazebo/Isaac Sim]\r\n    I --\x3e K\r\n    K --\x3e L[Sensors]\r\n    L --\x3e G\r\n    L --\x3e E\r\n    C --\x3e M[TTS Response]\r\n    M --\x3e N[Speaker]\n"})}),"\n",(0,t.jsx)(n.h2,{id:"phase-1-environment-setup",children:"Phase 1: Environment Setup"}),"\n",(0,t.jsx)(n.h3,{id:"create-a-home-environment",children:"Create a Home Environment"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Gazebo World"})," (",(0,t.jsx)(n.code,{children:"home_world.sdf"}),"):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\r\n<sdf version="1.8">\r\n  <world name="home">\r\n    \r\n    \x3c!-- Physics --\x3e\r\n    <physics name="1ms" type="ignored">\r\n      <max_step_size>0.001</max_step_size>\r\n      <real_time_factor>1.0</real_time_factor>\r\n    </physics>\r\n    \r\n    \x3c!-- Lighting --\x3e\r\n    <light type="directional" name="sun">\r\n      <pose>0 0 10 0 0 0</pose>\r\n      <diffuse>0.8 0.8 0.8 1</diffuse>\r\n      <direction>-0.5 0.1 -0.9</direction>\r\n    </light>\r\n    \r\n    \x3c!-- Ground --\x3e\r\n    <include>\r\n      <uri>model://ground_plane</uri>\r\n    </include>\r\n    \r\n    \x3c!-- Kitchen --\x3e\r\n    <model name="kitchen_table">\r\n      <static>true</static>\r\n      <pose>3 0 0 0 0 0</pose>\r\n      <link name="link">\r\n        <collision name="collision">\r\n          <geometry>\r\n            <box><size>1.5 0.8 0.75</size></box>\r\n          </geometry>\r\n        </collision>\r\n        <visual name="visual">\r\n          <geometry>\r\n            <box><size>1.5 0.8 0.75</size></box>\r\n          </geometry>\r\n          <material>\r\n            <ambient>0.6 0.4 0.2 1</ambient>\r\n          </material>\r\n        </visual>\r\n      </link>\r\n    </model>\r\n    \r\n    \x3c!-- Red Cup --\x3e\r\n    <model name="red_cup">\r\n      <pose>3 0 0.85 0 0 0</pose>\r\n      <link name="link">\r\n        <collision name="collision">\r\n          <geometry>\r\n            <cylinder><radius>0.04</radius><length>0.1</length></cylinder>\r\n          </geometry>\r\n        </collision>\r\n        <visual name="visual">\r\n          <geometry>\r\n            <cylinder><radius>0.04</radius><length>0.1</length></cylinder>\r\n          </geometry>\r\n          <material>\r\n            <ambient>0.8 0.0 0.0 1</ambient>\r\n          </material>\r\n        </visual>\r\n        <inertial>\r\n          <mass>0.1</mass>\r\n          <inertia>\r\n            <ixx>0.0001</ixx><iyy>0.0001</iyy><izz>0.00005</izz>\r\n          </inertia>\r\n        </inertial>\r\n      </link>\r\n    </model>\r\n    \r\n  </world>\r\n</sdf>\n'})}),"\n",(0,t.jsx)(n.h2,{id:"phase-2-perception-pipeline",children:"Phase 2: Perception Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"object-detection-with-yolo",children:"Object Detection with YOLO"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\r\nimport cv2\r\nfrom cv_bridge import CvBridge\r\n\r\nclass ObjectDetector(Node):\r\n    def __init__(self):\r\n        super().__init__('object_detector')\r\n        \r\n        # Load YOLO model\r\n        self.model = YOLO('yolov8n.pt')\r\n        \r\n        # ROS interfaces\r\n        self.bridge = CvBridge()\r\n        self.image_sub = self.create_subscription(\r\n            Image, '/camera/image_raw', self.image_callback, 10)\r\n        self.detection_pub = self.create_publisher(\r\n            String, '/detected_objects', 10)\r\n    \r\n    def image_callback(self, msg):\r\n        # Convert to OpenCV\r\n        cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\r\n        \r\n        # Run detection\r\n        results = self.model(cv_image)\r\n        \r\n        # Parse results\r\n        detections = []\r\n        for r in results:\r\n            for box in r.boxes:\r\n                cls = int(box.cls[0])\r\n                conf = float(box.conf[0])\r\n                name = self.model.names[cls]\r\n                \r\n                if conf > 0.5:\r\n                    detections.append({\r\n                        'name': name,\r\n                        'confidence': conf,\r\n                        'bbox': box.xyxy[0].tolist()\r\n                    })\r\n        \r\n        # Publish\r\n        msg = String()\r\n        msg.data = str(detections)\r\n        self.detection_pub.publish(msg)\r\n        \r\n        self.get_logger().info(f'Detected: {detections}')\n"})}),"\n",(0,t.jsx)(n.h3,{id:"visual-slam-for-localization",children:"Visual SLAM for Localization"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Launch ORB-SLAM3\r\nros2 run orb_slam3_ros2 stereo \\\r\n    ~/ORB_SLAM3/Vocabulary/ORBvoc.txt \\\r\n    ~/config/stereo_camera.yaml\n"})}),"\n",(0,t.jsx)(n.h2,{id:"phase-3-task-planning-with-llm",children:"Phase 3: Task Planning with LLM"}),"\n",(0,t.jsx)(n.h3,{id:"hierarchical-task-decomposition",children:"Hierarchical Task Decomposition"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import google.generativeai as genai\r\nimport json\r\n\r\nclass TaskPlanner:\r\n    def __init__(self):\r\n        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))\r\n        self.model = genai.GenerativeModel(\'gemini-1.5-flash\')\r\n    \r\n    def plan(self, user_command):\r\n        """\r\n        Decompose high-level command into robot primitives\r\n        """\r\n        prompt = f"""You are a robot task planner. Decompose the user\'s command into a sequence of primitive actions.\r\n\r\nAvailable primitives:\r\n- navigate(location): Move to a location\r\n- detect_object(object_name): Find an object using vision\r\n- pick(object_name): Grasp an object\r\n- place(location): Place held object\r\n- say(message): Speak to user\r\n\r\nUser command: "{user_command}"\r\n\r\nOutput a JSON array of actions with parameters. Example:\r\n[\r\n  {{"action": "navigate", "params": {{"location": "kitchen"}}}},\r\n  {{"action": "detect_object", "params": {{"object_name": "red cup"}}}},\r\n  {{"action": "pick", "params": {{"object_name": "red cup"}}}},\r\n  {{"action": "navigate", "params": {{"location": "user"}}}},\r\n  {{"action": "place", "params": {{"location": "table"}}}},\r\n  {{"action": "say", "params": {{"message": "Here is your red cup"}}}}\r\n]\r\n\r\nOnly output the JSON array, no other text."""\r\n\r\n        response = self.model.generate_content(prompt)\r\n        \r\n        # Parse JSON\r\n        try:\r\n            plan = json.loads(response.text)\r\n            return plan\r\n        except json.JSONDecodeError:\r\n            self.get_logger().error(f\'Failed to parse plan: {response.text}\')\r\n            return []\n'})}),"\n",(0,t.jsx)(n.h2,{id:"phase-4-action-execution",children:"Phase 4: Action Execution"}),"\n",(0,t.jsx)(n.h3,{id:"navigation-action",children:"Navigation Action"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from nav2_simple_commander.robot_navigator import BasicNavigator\r\nfrom geometry_msgs.msg import PoseStamped\r\n\r\nclass NavigationAction:\r\n    def __init__(self):\r\n        self.navigator = BasicNavigator()\r\n    \r\n    def execute(self, location):\r\n        \"\"\"\r\n        Navigate to a named location\r\n        \"\"\"\r\n        # Predefined locations\r\n        locations = {\r\n            'kitchen': (3.0, 0.0, 0.0),\r\n            'living_room': (0.0, 0.0, 0.0),\r\n            'bedroom': (0.0, 3.0, 0.0)\r\n        }\r\n        \r\n        if location not in locations:\r\n            return False\r\n        \r\n        # Create goal pose\r\n        goal_pose = PoseStamped()\r\n        goal_pose.header.frame_id = 'map'\r\n        goal_pose.header.stamp = self.navigator.get_clock().now().to_msg()\r\n        goal_pose.pose.position.x = locations[location][0]\r\n        goal_pose.pose.position.y = locations[location][1]\r\n        goal_pose.pose.orientation.w = 1.0\r\n        \r\n        # Navigate\r\n        self.navigator.goToPose(goal_pose)\r\n        \r\n        # Wait for completion\r\n        while not self.navigator.isTaskComplete():\r\n            feedback = self.navigator.getFeedback()\r\n            # Could add timeout here\r\n        \r\n        return self.navigator.getResult() == TaskResult.SUCCEEDED\n"})}),"\n",(0,t.jsx)(n.h3,{id:"manipulation-action-simplified",children:"Manipulation Action (Simplified)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class ManipulationAction:\r\n    def __init__(self):\r\n        self.arm_pub = self.create_publisher(\r\n            JointTrajectory, \'/arm_controller/joint_trajectory\', 10)\r\n    \r\n    def pick(self, object_name):\r\n        """\r\n        Pick up an object\r\n        """\r\n        # 1. Move arm to pre-grasp position\r\n        self.move_arm_to_position([0.0, -0.5, 0.5, 0.0, 0.0])\r\n        \r\n        # 2. Open gripper\r\n        self.set_gripper(open=True)\r\n        \r\n        # 3. Move to grasp position (would use object detection + IK)\r\n        self.move_arm_to_position([0.0, -0.3, 0.3, 0.0, 0.0])\r\n        \r\n        # 4. Close gripper\r\n        self.set_gripper(open=False)\r\n        \r\n        # 5. Lift\r\n        self.move_arm_to_position([0.0, -0.5, 0.5, 0.0, 0.0])\r\n        \r\n        return True\r\n    \r\n    def place(self, location):\r\n        """\r\n        Place held object\r\n        """\r\n        # 1. Move to place position\r\n        self.move_arm_to_position([0.0, -0.3, 0.3, 0.0, 0.0])\r\n        \r\n        # 2. Open gripper\r\n        self.set_gripper(open=True)\r\n        \r\n        # 3. Retract\r\n        self.move_arm_to_position([0.0, -0.5, 0.5, 0.0, 0.0])\r\n        \r\n        return True\n'})}),"\n",(0,t.jsx)(n.h2,{id:"phase-5-integration",children:"Phase 5: Integration"}),"\n",(0,t.jsx)(n.h3,{id:"main-controller",children:"Main Controller"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class AutonomousHumanoid(Node):\r\n    def __init__(self):\r\n        super().__init__('autonomous_humanoid')\r\n        \r\n        # Components\r\n        self.task_planner = TaskPlanner()\r\n        self.navigator = NavigationAction()\r\n        self.manipulator = ManipulationAction()\r\n        self.object_detector = ObjectDetector()\r\n        self.whisper = WhisperNode()\r\n        self.tts = TextToSpeech()\r\n        \r\n        # State\r\n        self.current_plan = []\r\n        self.current_step = 0\r\n    \r\n    def run(self):\r\n        \"\"\"\r\n        Main loop\r\n        \"\"\"\r\n        while rclpy.ok():\r\n            # 1. Listen for command\r\n            command = self.whisper.listen()\r\n            if not command:\r\n                continue\r\n            \r\n            self.get_logger().info(f'Received command: \"{command}\"')\r\n            self.tts.speak(\"I understand. Let me do that.\")\r\n            \r\n            # 2. Plan\r\n            self.current_plan = self.task_planner.plan(command)\r\n            self.get_logger().info(f'Plan: {self.current_plan}')\r\n            \r\n            # 3. Execute\r\n            for step in self.current_plan:\r\n                success = self.execute_action(step)\r\n                if not success:\r\n                    self.tts.speak(\"Sorry, I encountered an error.\")\r\n                    break\r\n            else:\r\n                self.tts.speak(\"Task completed successfully!\")\r\n    \r\n    def execute_action(self, action):\r\n        \"\"\"\r\n        Execute a single action\r\n        \"\"\"\r\n        action_type = action['action']\r\n        params = action['params']\r\n        \r\n        self.get_logger().info(f'Executing: {action_type} with {params}')\r\n        \r\n        if action_type == 'navigate':\r\n            return self.navigator.execute(params['location'])\r\n        \r\n        elif action_type == 'detect_object':\r\n            # Wait for object detection\r\n            # In practice, would actively search\r\n            return True\r\n        \r\n        elif action_type == 'pick':\r\n            return self.manipulator.pick(params['object_name'])\r\n        \r\n        elif action_type == 'place':\r\n            return self.manipulator.place(params['location'])\r\n        \r\n        elif action_type == 'say':\r\n            self.tts.speak(params['message'])\r\n            return True\r\n        \r\n        else:\r\n            self.get_logger().error(f'Unknown action: {action_type}')\r\n            return False\n"})}),"\n",(0,t.jsx)(n.h2,{id:"phase-6-testing-and-validation",children:"Phase 6: Testing and Validation"}),"\n",(0,t.jsx)(n.h3,{id:"test-cases",children:"Test Cases"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Simple Navigation"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Command: "Go to the kitchen"'}),"\n",(0,t.jsx)(n.li,{children:"Expected: Robot navigates to kitchen"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Object Retrieval"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Command: "Bring me the red cup from the kitchen"'}),"\n",(0,t.jsx)(n.li,{children:"Expected: Navigate \u2192 Detect \u2192 Pick \u2192 Navigate \u2192 Place"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Multi-Step Task"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Command: "Clean the table in the kitchen"'}),"\n",(0,t.jsx)(n.li,{children:"Expected: Navigate \u2192 Detect objects \u2192 Pick each \u2192 Place in bin"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Metric"}),(0,t.jsx)(n.th,{children:"Target"}),(0,t.jsx)(n.th,{children:"Measurement"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Navigation Success Rate"})}),(0,t.jsx)(n.td,{children:">90%"}),(0,t.jsx)(n.td,{children:"% of successful navigations"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Object Detection Accuracy"})}),(0,t.jsx)(n.td,{children:">85%"}),(0,t.jsx)(n.td,{children:"Precision/Recall"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Manipulation Success Rate"})}),(0,t.jsx)(n.td,{children:">80%"}),(0,t.jsx)(n.td,{children:"% of successful grasps"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"End-to-End Task Success"})}),(0,t.jsx)(n.td,{children:">75%"}),(0,t.jsx)(n.td,{children:"% of fully completed tasks"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Speech Recognition Accuracy"})}),(0,t.jsx)(n.td,{children:">95%"}),(0,t.jsx)(n.td,{children:"Word Error Rate (WER)"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"submission-requirements",children:"Submission Requirements"}),"\n",(0,t.jsx)(n.h3,{id:"1-github-repository",children:"1. GitHub Repository"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"capstone-humanoid-robot/\r\n\u251c\u2500\u2500 README.md\r\n\u251c\u2500\u2500 src/\r\n\u2502   \u251c\u2500\u2500 perception/\r\n\u2502   \u2502   \u251c\u2500\u2500 object_detector.py\r\n\u2502   \u2502   \u2514\u2500\u2500 vslam_node.py\r\n\u2502   \u251c\u2500\u2500 planning/\r\n\u2502   \u2502   \u2514\u2500\u2500 task_planner.py\r\n\u2502   \u251c\u2500\u2500 control/\r\n\u2502   \u2502   \u251c\u2500\u2500 navigation_action.py\r\n\u2502   \u2502   \u2514\u2500\u2500 manipulation_action.py\r\n\u2502   \u251c\u2500\u2500 interaction/\r\n\u2502   \u2502   \u251c\u2500\u2500 whisper_node.py\r\n\u2502   \u2502   \u2514\u2500\u2500 tts_node.py\r\n\u2502   \u2514\u2500\u2500 main_controller.py\r\n\u251c\u2500\u2500 launch/\r\n\u2502   \u2514\u2500\u2500 capstone.launch.py\r\n\u251c\u2500\u2500 config/\r\n\u2502   \u251c\u2500\u2500 nav2_params.yaml\r\n\u2502   \u2514\u2500\u2500 robot_description.urdf\r\n\u251c\u2500\u2500 worlds/\r\n\u2502   \u2514\u2500\u2500 home_world.sdf\r\n\u2514\u2500\u2500 docs/\r\n    \u251c\u2500\u2500 architecture.md\r\n    \u2514\u2500\u2500 demo_video.mp4\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-demo-video-90-seconds-max",children:"2. Demo Video (90 seconds max)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Structure"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"0-10s: Introduction and scenario"}),"\n",(0,t.jsx)(n.li,{children:"10-30s: Voice command demonstration"}),"\n",(0,t.jsx)(n.li,{children:"30-60s: Robot execution (navigation, detection, manipulation)"}),"\n",(0,t.jsx)(n.li,{children:"60-80s: Task completion and robot response"}),"\n",(0,t.jsx)(n.li,{children:"80-90s: Technical highlights and conclusion"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Tips"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use screen recording (OBS Studio)"}),"\n",(0,t.jsx)(n.li,{children:"Add subtitles for voice commands"}),"\n",(0,t.jsx)(n.li,{children:"Show RViz visualization alongside Gazebo"}),"\n",(0,t.jsx)(n.li,{children:"Highlight key moments (object detection, successful grasp)"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-documentation",children:"3. Documentation"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"README.md"})," should include:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Project overview"}),"\n",(0,t.jsx)(n.li,{children:"System architecture diagram"}),"\n",(0,t.jsx)(n.li,{children:"Installation instructions"}),"\n",(0,t.jsx)(n.li,{children:"How to run the demo"}),"\n",(0,t.jsx)(n.li,{children:"Technical challenges and solutions"}),"\n",(0,t.jsx)(n.li,{children:"Future improvements"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"evaluation-rubric",children:"Evaluation Rubric"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Category"}),(0,t.jsx)(n.th,{children:"Points"}),(0,t.jsx)(n.th,{children:"Criteria"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Functionality"})}),(0,t.jsx)(n.td,{children:"40"}),(0,t.jsx)(n.td,{children:"Does it work end-to-end?"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Code Quality"})}),(0,t.jsx)(n.td,{children:"20"}),(0,t.jsx)(n.td,{children:"Clean, documented, modular"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Innovation"})}),(0,t.jsx)(n.td,{children:"20"}),(0,t.jsx)(n.td,{children:"Novel features, creative solutions"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Documentation"})}),(0,t.jsx)(n.td,{children:"10"}),(0,t.jsx)(n.td,{children:"Clear README, architecture docs"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Demo Video"})}),(0,t.jsx)(n.td,{children:"10"}),(0,t.jsx)(n.td,{children:"Professional, concise, engaging"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"bonus-challenges",children:"Bonus Challenges"}),"\n",(0,t.jsx)(n.h3,{id:"challenge-1-multi-robot-coordination",children:"Challenge 1: Multi-Robot Coordination"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Deploy two humanoids that collaborate on a task"}),"\n",(0,t.jsx)(n.li,{children:"Example: One holds a box while the other places objects inside"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"challenge-2-adaptive-behavior",children:"Challenge 2: Adaptive Behavior"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Robot learns from failures and retries with different strategies"}),"\n",(0,t.jsx)(n.li,{children:"Example: If grasp fails, try different approach angle"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"challenge-3-real-world-deployment",children:"Challenge 3: Real-World Deployment"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Deploy on a physical robot (Unitree G1, custom build)"}),"\n",(0,t.jsx)(n.li,{children:"Document sim-to-real transfer process"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"final-thoughts",children:"Final Thoughts"}),"\n",(0,t.jsx)(n.p,{children:"This capstone project synthesizes everything you've learned:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2"})," for system architecture"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Gazebo/Isaac Sim"})," for simulation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Nav2"})," for navigation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Computer Vision"})," for perception"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LLMs"})," for reasoning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Control Theory"})," for manipulation"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"You are now equipped to build the next generation of humanoid robots."})}),"\n",(0,t.jsx)(n.h2,{id:"submission",children:"Submission"}),"\n",(0,t.jsxs)(n.p,{children:["Submit your project here: ",(0,t.jsx)(n.a,{href:"https://forms.gle/CQsSEGM3GeCrL43c8",children:"Hackathon Submission Form"})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Required"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Public GitHub repository link"}),"\n",(0,t.jsx)(n.li,{children:"Deployed book link (GitHub Pages/Vercel)"}),"\n",(0,t.jsx)(n.li,{children:"Demo video link (YouTube/Vimeo, <90 seconds)"}),"\n",(0,t.jsx)(n.li,{children:"WhatsApp number for presentation invitation"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Deadline"}),": Sunday, Nov 30, 2025 at 6:00 PM"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"congratulations",children:"Congratulations!"}),"\n",(0,t.jsx)(n.p,{children:"You've completed the Physical AI & Humanoid Robotics course. You now have the skills to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Design and simulate humanoid robots"}),"\n",(0,t.jsx)(n.li,{children:"Implement perception, planning, and control systems"}),"\n",(0,t.jsx)(n.li,{children:"Integrate AI models for natural interaction"}),"\n",(0,t.jsx)(n.li,{children:"Deploy autonomous robots in complex environments"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"The future of robotics is in your hands. Build something amazing!"})," \ud83e\udd16\ud83d\ude80"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>l});var i=r(6540);const t={},o=i.createContext(t);function s(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);