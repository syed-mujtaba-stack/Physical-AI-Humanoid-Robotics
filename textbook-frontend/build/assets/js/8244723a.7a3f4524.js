"use strict";(globalThis.webpackChunktextbook_frontend=globalThis.webpackChunktextbook_frontend||[]).push([[1990],{5079:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapter-10-conversational","title":"Chapter 10: Conversational Robotics with LLMs","description":"Learning Objectives","source":"@site/docs/chapter-10-conversational.md","sourceDirName":".","slug":"/chapter-10-conversational","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter-10-conversational","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter-10-conversational.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"sidebar_position":11},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 9: Control & Balance for Bipedal Robots","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter-09-control"},"next":{"title":"Chapter 11: Capstone Project - The Autonomous Humanoid","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter-11-capstone"}}');var i=r(4848),t=r(8453);const o={sidebar_position:11},a="Chapter 10: Conversational Robotics with LLMs",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"The Rise of Conversational Robots",id:"the-rise-of-conversational-robots",level:2},{value:"Why Now?",id:"why-now",level:3},{value:"Speech-to-Text with Whisper",id:"speech-to-text-with-whisper",level:2},{value:"Features",id:"features",level:3},{value:"Installation",id:"installation",level:3},{value:"Basic Usage",id:"basic-usage",level:3},{value:"Real-Time Transcription",id:"real-time-transcription",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:3},{value:"Language Models for Robot Control",id:"language-models-for-robot-control",level:2},{value:"GPT-4 for Task Planning",id:"gpt-4-for-task-planning",level:3},{value:"Gemini for Vision + Language",id:"gemini-for-vision--language",level:3},{value:"Text-to-Speech",id:"text-to-speech",level:2},{value:"Using pyttsx3 (Offline)",id:"using-pyttsx3-offline",level:3},{value:"Using Google Cloud TTS (Online, High Quality)",id:"using-google-cloud-tts-online-high-quality",level:3},{value:"Building a Conversational Robot System",id:"building-a-conversational-robot-system",level:2},{value:"Architecture",id:"architecture",level:3},{value:"Complete System",id:"complete-system",level:3},{value:"Context-Aware Conversations",id:"context-aware-conversations",level:2},{value:"Maintaining Conversation History",id:"maintaining-conversation-history",level:3},{value:"Lab Exercise: Voice-Controlled Navigation",id:"lab-exercise-voice-controlled-navigation",level:2},{value:"Objective",id:"objective",level:3},{value:"Step 1: Set Up Whisper Node",id:"step-1-set-up-whisper-node",level:3},{value:"Step 2: Set Up Command Interpreter",id:"step-2-set-up-command-interpreter",level:3},{value:"Step 3: Test",id:"step-3-test",level:3},{value:"Quiz",id:"quiz",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-10-conversational-robotics-with-llms",children:"Chapter 10: Conversational Robotics with LLMs"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate speech recognition (Whisper) with humanoid robots"}),"\n",(0,i.jsx)(n.li,{children:"Use Large Language Models (GPT-4, Gemini) for natural language understanding"}),"\n",(0,i.jsx)(n.li,{children:"Implement text-to-speech for robot responses"}),"\n",(0,i.jsx)(n.li,{children:"Build multi-modal interaction systems (speech + vision + gesture)"}),"\n",(0,i.jsx)(n.li,{children:"Deploy conversational AI on edge devices (Jetson)"}),"\n",(0,i.jsx)(n.li,{children:"Create context-aware robot assistants"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"the-rise-of-conversational-robots",children:"The Rise of Conversational Robots"}),"\n",(0,i.jsx)(n.p,{children:"Traditional robots use:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Button interfaces"}),": Limited, unintuitive"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pre-programmed commands"}),": Inflexible"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Mobile apps"}),": Requires phone"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Conversational robots"})," use ",(0,i.jsx)(n.strong,{children:"natural language"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'"Robot, bring me a glass of water"'}),"\n",(0,i.jsx)(n.li,{children:'"What do you see on the table?"'}),"\n",(0,i.jsx)(n.li,{children:'"Follow me to the kitchen"'}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"why-now",children:"Why Now?"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Technology"}),(0,i.jsx)(n.th,{children:"Breakthrough"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Speech Recognition"})}),(0,i.jsx)(n.td,{children:"Whisper (OpenAI) - near-human accuracy"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Language Models"})}),(0,i.jsx)(n.td,{children:"GPT-4, Gemini - reasoning and planning"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Text-to-Speech"})}),(0,i.jsx)(n.td,{children:"Eleven Labs, Coqui TTS - natural voices"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Edge AI"})}),(0,i.jsx)(n.td,{children:"Jetson Orin - run models locally"})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"speech-to-text-with-whisper",children:"Speech-to-Text with Whisper"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Whisper"})," (OpenAI, 2022) is a robust speech recognition model."]}),"\n",(0,i.jsx)(n.h3,{id:"features",children:"Features"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multilingual"}),": 99 languages"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robust"}),": Works with noise, accents"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fast"}),": Real-time on GPU"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Open-source"}),": Can run locally"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper\n"})}),"\n",(0,i.jsx)(n.h3,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import whisper\r\n\r\n# Load model\r\nmodel = whisper.load_model("base")  # Options: tiny, base, small, medium, large\r\n\r\n# Transcribe audio file\r\nresult = model.transcribe("audio.wav")\r\nprint(result["text"])\n'})}),"\n",(0,i.jsx)(n.h3,{id:"real-time-transcription",children:"Real-Time Transcription"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import pyaudio\r\nimport whisper\r\nimport numpy as np\r\n\r\nclass RealtimeWhisper:\r\n    def __init__(self):\r\n        self.model = whisper.load_model("base")\r\n        self.audio = pyaudio.PyAudio()\r\n        self.stream = self.audio.open(\r\n            format=pyaudio.paInt16,\r\n            channels=1,\r\n            rate=16000,\r\n            input=True,\r\n            frames_per_buffer=1024\r\n        )\r\n    \r\n    def listen(self, duration=5):\r\n        """Record audio for \'duration\' seconds and transcribe"""\r\n        print("Listening...")\r\n        \r\n        frames = []\r\n        for _ in range(0, int(16000 / 1024 * duration)):\r\n            data = self.stream.read(1024)\r\n            frames.append(data)\r\n        \r\n        # Convert to numpy array\r\n        audio_data = np.frombuffer(b\'\'.join(frames), dtype=np.int16).astype(np.float32) / 32768.0\r\n        \r\n        # Transcribe\r\n        result = self.model.transcribe(audio_data, fp16=False)\r\n        return result["text"]\n'})}),"\n",(0,i.jsx)(n.h3,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\n\r\nclass WhisperNode(Node):\r\n    def __init__(self):\r\n        super().__init__('whisper_node')\r\n        self.publisher = self.create_publisher(String, 'voice_command', 10)\r\n        self.whisper = RealtimeWhisper()\r\n        \r\n        # Listen every 5 seconds\r\n        self.timer = self.create_timer(5.0, self.listen_callback)\r\n    \r\n    def listen_callback(self):\r\n        text = self.whisper.listen(duration=5)\r\n        if text:\r\n            msg = String()\r\n            msg.data = text\r\n            self.publisher.publish(msg)\r\n            self.get_logger().info(f'Heard: \"{text}\"')\n"})}),"\n",(0,i.jsx)(n.h2,{id:"language-models-for-robot-control",children:"Language Models for Robot Control"}),"\n",(0,i.jsx)(n.h3,{id:"gpt-4-for-task-planning",children:"GPT-4 for Task Planning"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\r\n\r\nclient = OpenAI(api_key="your-api-key")\r\n\r\ndef plan_task(user_command):\r\n    """\r\n    Convert natural language to robot actions\r\n    """\r\n    prompt = f"""You are a robot assistant. Convert the user\'s command into a sequence of robot actions.\r\n\r\nAvailable actions:\r\n- navigate(location)\r\n- pick(object)\r\n- place(object, location)\r\n- say(message)\r\n\r\nUser command: "{user_command}"\r\n\r\nOutput a JSON list of actions."""\r\n\r\n    response = client.chat.completions.create(\r\n        model="gpt-4",\r\n        messages=[\r\n            {"role": "system", "content": "You are a helpful robot assistant."},\r\n            {"role": "user", "content": prompt}\r\n        ]\r\n    )\r\n    \r\n    return response.choices[0].message.content\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'command = "Bring me the red cup from the kitchen"\r\nplan = plan_task(command)\r\nprint(plan)\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Output"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'[\r\n  {"action": "navigate", "params": {"location": "kitchen"}},\r\n  {"action": "pick", "params": {"object": "red cup"}},\r\n  {"action": "navigate", "params": {"location": "user"}},\r\n  {"action": "place", "params": {"object": "red cup", "location": "table"}},\r\n  {"action": "say", "params": {"message": "Here is your red cup"}}\r\n]\n'})}),"\n",(0,i.jsx)(n.h3,{id:"gemini-for-vision--language",children:"Gemini for Vision + Language"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import google.generativeai as genai\r\nfrom PIL import Image\r\n\r\ngenai.configure(api_key="your-api-key")\r\n\r\ndef vision_language_query(image_path, question):\r\n    """\r\n    Ask questions about an image\r\n    """\r\n    model = genai.GenerativeModel(\'gemini-1.5-flash\')\r\n    \r\n    image = Image.open(image_path)\r\n    \r\n    response = model.generate_content([question, image])\r\n    return response.text\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'response = vision_language_query("table.jpg", "What objects are on the table?")\r\nprint(response)\r\n# Output: "I see a red cup, a blue plate, and a green apple on the table."\n'})}),"\n",(0,i.jsx)(n.h2,{id:"text-to-speech",children:"Text-to-Speech"}),"\n",(0,i.jsx)(n.h3,{id:"using-pyttsx3-offline",children:"Using pyttsx3 (Offline)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import pyttsx3\r\n\r\nclass TextToSpeech:\r\n    def __init__(self):\r\n        self.engine = pyttsx3.init()\r\n        self.engine.setProperty('rate', 150)  # Speed\r\n        self.engine.setProperty('volume', 0.9)  # Volume\r\n    \r\n    def speak(self, text):\r\n        self.engine.say(text)\r\n        self.engine.runAndWait()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"using-google-cloud-tts-online-high-quality",children:"Using Google Cloud TTS (Online, High Quality)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from google.cloud import texttospeech\r\nimport os\r\n\r\nclass GoogleTTS:\r\n    def __init__(self):\r\n        self.client = texttospeech.TextToSpeechClient()\r\n    \r\n    def speak(self, text, output_file="output.mp3"):\r\n        synthesis_input = texttospeech.SynthesisInput(text=text)\r\n        \r\n        voice = texttospeech.VoiceSelectionParams(\r\n            language_code="en-US",\r\n            name="en-US-Neural2-F",  # Female voice\r\n            ssml_gender=texttospeech.SsmlVoiceGender.FEMALE\r\n        )\r\n        \r\n        audio_config = texttospeech.AudioConfig(\r\n            audio_encoding=texttospeech.AudioEncoding.MP3\r\n        )\r\n        \r\n        response = self.client.synthesize_speech(\r\n            input=synthesis_input,\r\n            voice=voice,\r\n            audio_config=audio_config\r\n        )\r\n        \r\n        with open(output_file, "wb") as out:\r\n            out.write(response.audio_content)\r\n        \r\n        # Play audio\r\n        os.system(f"mpg123 {output_file}")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"building-a-conversational-robot-system",children:"Building a Conversational Robot System"}),"\n",(0,i.jsx)(n.h3,{id:"architecture",children:"Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:"graph LR\r\n    A[Microphone] --\x3e B[Whisper STT]\r\n    B --\x3e C[GPT-4 / Gemini]\r\n    D[Camera] --\x3e C\r\n    C --\x3e E[Action Planner]\r\n    E --\x3e F[Robot Controller]\r\n    C --\x3e G[TTS]\r\n    G --\x3e H[Speaker]\n"})}),"\n",(0,i.jsx)(n.h3,{id:"complete-system",children:"Complete System"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom sensor_msgs.msg import Image\r\nfrom geometry_msgs.msg import Twist\r\nimport whisper\r\nimport google.generativeai as genai\r\nimport pyttsx3\r\n\r\nclass ConversationalRobot(Node):\r\n    def __init__(self):\r\n        super().__init__(\'conversational_robot\')\r\n        \r\n        # Components\r\n        self.whisper_model = whisper.load_model("base")\r\n        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))\r\n        self.gemini = genai.GenerativeModel(\'gemini-1.5-flash\')\r\n        self.tts = pyttsx3.init()\r\n        \r\n        # ROS interfaces\r\n        self.cmd_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'/camera/image_raw\', self.image_callback, 10)\r\n        \r\n        self.current_image = None\r\n        \r\n        # Conversation loop\r\n        self.timer = self.create_timer(10.0, self.conversation_loop)\r\n    \r\n    def image_callback(self, msg):\r\n        # Store latest image\r\n        self.current_image = msg\r\n    \r\n    def conversation_loop(self):\r\n        # 1. Listen\r\n        user_command = self.listen()\r\n        if not user_command:\r\n            return\r\n        \r\n        self.get_logger().info(f\'User said: "{user_command}"\')\r\n        \r\n        # 2. Understand (with vision if needed)\r\n        if "see" in user_command.lower() or "look" in user_command.lower():\r\n            response = self.vision_language_query(user_command)\r\n        else:\r\n            response = self.text_query(user_command)\r\n        \r\n        self.get_logger().info(f\'Robot response: "{response}"\')\r\n        \r\n        # 3. Speak\r\n        self.speak(response)\r\n        \r\n        # 4. Act (if command involves movement)\r\n        if "forward" in user_command.lower():\r\n            self.move_forward()\r\n        elif "turn" in user_command.lower():\r\n            self.turn()\r\n    \r\n    def listen(self):\r\n        # Simplified: record 5 seconds\r\n        # In practice, use voice activity detection (VAD)\r\n        audio_data = self.record_audio(duration=5)\r\n        result = self.whisper_model.transcribe(audio_data)\r\n        return result["text"]\r\n    \r\n    def text_query(self, question):\r\n        response = self.gemini.generate_content(question)\r\n        return response.text\r\n    \r\n    def vision_language_query(self, question):\r\n        if self.current_image is None:\r\n            return "I don\'t have a camera feed."\r\n        \r\n        # Convert ROS image to PIL\r\n        image = self.ros_image_to_pil(self.current_image)\r\n        \r\n        response = self.gemini.generate_content([question, image])\r\n        return response.text\r\n    \r\n    def speak(self, text):\r\n        self.tts.say(text)\r\n        self.tts.runAndWait()\r\n    \r\n    def move_forward(self):\r\n        cmd = Twist()\r\n        cmd.linear.x = 0.5\r\n        self.cmd_pub.publish(cmd)\r\n        self.get_logger().info(\'Moving forward\')\r\n    \r\n    def turn(self):\r\n        cmd = Twist()\r\n        cmd.angular.z = 0.5\r\n        self.cmd_pub.publish(cmd)\r\n        self.get_logger().info(\'Turning\')\n'})}),"\n",(0,i.jsx)(n.h2,{id:"context-aware-conversations",children:"Context-Aware Conversations"}),"\n",(0,i.jsx)(n.h3,{id:"maintaining-conversation-history",children:"Maintaining Conversation History"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class ContextAwareRobot:\r\n    def __init__(self):\r\n        self.conversation_history = []\r\n    \r\n    def chat(self, user_input):\r\n        # Add user message to history\r\n        self.conversation_history.append({\r\n            "role": "user",\r\n            "content": user_input\r\n        })\r\n        \r\n        # Query LLM with full history\r\n        response = client.chat.completions.create(\r\n            model="gpt-4",\r\n            messages=[\r\n                {"role": "system", "content": "You are a helpful robot assistant."},\r\n                *self.conversation_history\r\n            ]\r\n        )\r\n        \r\n        assistant_message = response.choices[0].message.content\r\n        \r\n        # Add assistant response to history\r\n        self.conversation_history.append({\r\n            "role": "assistant",\r\n            "content": assistant_message\r\n        })\r\n        \r\n        return assistant_message\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'robot = ContextAwareRobot()\r\n\r\nprint(robot.chat("What\'s on the table?"))\r\n# "I see a red cup and a blue plate."\r\n\r\nprint(robot.chat("Pick up the red one"))\r\n# "Picking up the red cup." (understands "red one" refers to the cup)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"lab-exercise-voice-controlled-navigation",children:"Lab Exercise: Voice-Controlled Navigation"}),"\n",(0,i.jsx)(n.h3,{id:"objective",children:"Objective"}),"\n",(0,i.jsx)(n.p,{children:"Build a robot that navigates based on voice commands."}),"\n",(0,i.jsx)(n.h3,{id:"step-1-set-up-whisper-node",children:"Step 1: Set Up Whisper Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 run conversational_robot whisper_node\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-2-set-up-command-interpreter",children:"Step 2: Set Up Command Interpreter"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class VoiceNavigationNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'voice_navigation\')\r\n        self.subscription = self.create_subscription(\r\n            String, \'voice_command\', self.command_callback, 10)\r\n        self.cmd_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n    \r\n    def command_callback(self, msg):\r\n        command = msg.data.lower()\r\n        \r\n        if "forward" in command or "ahead" in command:\r\n            self.move_forward()\r\n        elif "back" in command or "backward" in command:\r\n            self.move_backward()\r\n        elif "left" in command:\r\n            self.turn_left()\r\n        elif "right" in command:\r\n            self.turn_right()\r\n        elif "stop" in command:\r\n            self.stop()\r\n    \r\n    def move_forward(self):\r\n        cmd = Twist()\r\n        cmd.linear.x = 0.5\r\n        self.cmd_pub.publish(cmd)\r\n    \r\n    # ... other movement methods\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-3-test",children:"Step 3: Test"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Terminal 1: Launch Gazebo\r\nros2 launch humanoid_gazebo world.launch.py\r\n\r\n# Terminal 2: Whisper node\r\nros2 run conversational_robot whisper_node\r\n\r\n# Terminal 3: Navigation node\r\nros2 run conversational_robot voice_navigation_node\r\n\r\n# Speak: "Move forward"\n'})}),"\n",(0,i.jsx)(n.h2,{id:"quiz",children:"Quiz"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What is Whisper used for?"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A) Text generation"}),"\n",(0,i.jsx)(n.li,{children:"B) Speech-to-text"}),"\n",(0,i.jsx)(n.li,{children:"C) Image recognition"}),"\n",(0,i.jsx)(n.li,{children:"D) Robot control"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Answer: B"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Which model is best for vision + language tasks?"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A) GPT-3"}),"\n",(0,i.jsx)(n.li,{children:"B) BERT"}),"\n",(0,i.jsx)(n.li,{children:"C) Gemini 1.5 Flash"}),"\n",(0,i.jsx)(n.li,{children:"D) ResNet"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Answer: C"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why maintain conversation history?"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A) To save memory"}),"\n",(0,i.jsx)(n.li,{children:"B) To enable context-aware responses"}),"\n",(0,i.jsx)(n.li,{children:"C) To speed up inference"}),"\n",(0,i.jsx)(n.li,{children:"D) To reduce API costs"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Answer: B"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What is the advantage of running TTS locally?"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A) Better voice quality"}),"\n",(0,i.jsx)(n.li,{children:"B) No internet required, lower latency"}),"\n",(0,i.jsx)(n.li,{children:"C) More languages"}),"\n",(0,i.jsx)(n.li,{children:"D) Cheaper"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Answer: B"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"In this chapter, we integrated conversational AI with humanoid robots. We used Whisper for speech recognition, GPT-4/Gemini for natural language understanding and task planning, and TTS for robot responses. We built a complete conversational robot system that can understand voice commands, answer questions about its environment, and execute tasks. Conversational interfaces make robots accessible to everyone, not just programmers."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Next Chapter"}),": We'll bring everything together in the capstone project\u2014building an autonomous humanoid that navigates, manipulates objects, and responds to voice commands."]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var s=r(6540);const i={},t=s.createContext(i);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);