"use strict";(globalThis.webpackChunktextbook_frontend=globalThis.webpackChunktextbook_frontend||[]).push([[1433],{350:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"chapter-08-vla","title":"Chapter 8: Vision-Language-Action (VLA) Models","description":"Learning Objectives","source":"@site/docs/chapter-08-vla.md","sourceDirName":".","slug":"/chapter-08-vla","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter-08-vla","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter-08-vla.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 7: VSLAM & Navigation with Nav2","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter-07-vslam"},"next":{"title":"Chapter 9: Control & Balance for Bipedal Robots","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter-09-control"}}');var s=r(4848),l=r(8453);const o={sidebar_position:9},t="Chapter 8: Vision-Language-Action (VLA) Models",a={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"The VLA Revolution",id:"the-vla-revolution",level:2},{value:"Foundation Models in Robotics",id:"foundation-models-in-robotics",level:2},{value:"RT-1: Robotics Transformer",id:"rt-1-robotics-transformer",level:2},{value:"Architecture",id:"architecture",level:3},{value:"Training Data",id:"training-data",level:3},{value:"Performance",id:"performance",level:3},{value:"RT-2: Vision-Language-Action with VLMs",id:"rt-2-vision-language-action-with-vlms",level:2},{value:"Key Innovation",id:"key-innovation",level:3},{value:"Results",id:"results",level:3},{value:"Implementing a Simple VLA Model",id:"implementing-a-simple-vla-model",level:2},{value:"Step 1: Data Collection",id:"step-1-data-collection",level:3},{value:"Step 2: Model Architecture (PyTorch)",id:"step-2-model-architecture-pytorch",level:3},{value:"Step 3: Training Loop",id:"step-3-training-loop",level:3},{value:"Step 4: Inference on Robot",id:"step-4-inference-on-robot",level:3},{value:"Open-Source VLA Models",id:"open-source-vla-models",level:2},{value:"OpenVLA (Stanford, 2024)",id:"openvla-stanford-2024",level:3},{value:"Octo (UC Berkeley, 2024)",id:"octo-uc-berkeley-2024",level:3},{value:"Challenges and Limitations",id:"challenges-and-limitations",level:2},{value:"1. Data Hunger",id:"1-data-hunger",level:3},{value:"2. Safety",id:"2-safety",level:3},{value:"3. Computational Cost",id:"3-computational-cost",level:3},{value:"Lab Exercise: VLA for Humanoid Navigation",id:"lab-exercise-vla-for-humanoid-navigation",level:2},{value:"Objective",id:"objective",level:3},{value:"Step 1: Collect Data",id:"step-1-collect-data",level:3},{value:"Step 2: Train Model",id:"step-2-train-model",level:3},{value:"Step 3: Deploy",id:"step-3-deploy",level:3},{value:"Quiz",id:"quiz",level:2},{value:"Summary",id:"summary",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-8-vision-language-action-vla-models",children:"Chapter 8: Vision-Language-Action (VLA) Models"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the VLA paradigm shift in robotics"}),"\n",(0,s.jsx)(n.li,{children:"Explore RT-1, RT-2, and other VLA architectures"}),"\n",(0,s.jsx)(n.li,{children:"Implement end-to-end vision-to-action pipelines"}),"\n",(0,s.jsx)(n.li,{children:"Fine-tune VLA models for custom tasks"}),"\n",(0,s.jsx)(n.li,{children:"Deploy VLA models on humanoid robots"}),"\n",(0,s.jsx)(n.li,{children:"Understand the role of foundation models in robotics"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"the-vla-revolution",children:"The VLA Revolution"}),"\n",(0,s.jsxs)(n.p,{children:["Traditional robotics pipelines are ",(0,s.jsx)(n.strong,{children:"modular"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Camera \u2192 Object Detection \u2192 Pose Estimation \u2192 Motion Planning \u2192 Control\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problems"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Each module has errors that compound"}),"\n",(0,s.jsx)(n.li,{children:"Requires manual engineering for each task"}),"\n",(0,s.jsx)(n.li,{children:"Doesn't generalize to new scenarios"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"VLA Models"})," are ",(0,s.jsx)(n.strong,{children:"end-to-end"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Camera Image + Language Command \u2192 Motor Commands\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Advantages"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Single neural network (no error propagation)"}),"\n",(0,s.jsx)(n.li,{children:"Learns from data (less manual engineering)"}),"\n",(0,s.jsx)(n.li,{children:"Generalizes via pre-training on large datasets"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"foundation-models-in-robotics",children:"Foundation Models in Robotics"}),"\n",(0,s.jsxs)(n.p,{children:["VLA models leverage ",(0,s.jsx)(n.strong,{children:"foundation models"})," pre-trained on internet-scale data:"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Pre-training Data"}),(0,s.jsx)(n.th,{children:"Robotics Application"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"CLIP"})}),(0,s.jsx)(n.td,{children:"400M image-text pairs"}),(0,s.jsx)(n.td,{children:"Visual grounding"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"GPT-4"})}),(0,s.jsx)(n.td,{children:"Trillions of tokens"}),(0,s.jsx)(n.td,{children:"Task planning, reasoning"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"SAM"})}),(0,s.jsx)(n.td,{children:"1B masks"}),(0,s.jsx)(n.td,{children:"Object segmentation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"DINOv2"})}),(0,s.jsx)(n.td,{children:"142M images"}),(0,s.jsx)(n.td,{children:"Visual features"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Insight"}),": Pre-training on diverse data enables ",(0,s.jsx)(n.strong,{children:"zero-shot generalization"})," to new objects and tasks."]}),"\n",(0,s.jsx)(n.h2,{id:"rt-1-robotics-transformer",children:"RT-1: Robotics Transformer"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"RT-1"})," (Google DeepMind, 2022) was the first large-scale VLA model."]}),"\n",(0,s.jsx)(n.h3,{id:"architecture",children:"Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'Input: RGB Image (320x256) + Language Command ("pick up the apple")\r\n       \u2193\r\n    Vision Encoder (EfficientNet)\r\n       \u2193\r\n    Language Encoder (Universal Sentence Encoder)\r\n       \u2193\r\n    Transformer (8 layers)\r\n       \u2193\r\n    Action Tokens (7-DOF arm + gripper)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"training-data",children:"Training Data"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"130,000 episodes"})," from 13 robots"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"700 tasks"})," (pick, place, push, open drawer, etc.)"]}),"\n",(0,s.jsx)(n.li,{children:"Real-world data (no simulation)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance",children:"Performance"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"97% success"})," on seen tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"76% success"})," on novel objects (zero-shot)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'Command: "Pick up the blue block"\r\nRT-1 Output: [x, y, z, roll, pitch, yaw, gripper_open]\n'})}),"\n",(0,s.jsx)(n.h2,{id:"rt-2-vision-language-action-with-vlms",children:"RT-2: Vision-Language-Action with VLMs"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"RT-2"})," (2023) improves RT-1 by using ",(0,s.jsx)(n.strong,{children:"Vision-Language Models (VLMs)"})," like PaLM-E."]}),"\n",(0,s.jsx)(n.h3,{id:"key-innovation",children:"Key Innovation"}),"\n",(0,s.jsxs)(n.p,{children:["Pre-train on ",(0,s.jsx)(n.strong,{children:"web-scale vision-language data"}),", then fine-tune on robot data."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Training Pipeline"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pre-training"}),": PaLM-E on 1B image-text pairs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Co-fine-tuning"}),": Mix web data + robot data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Head"}),": Add output layer for motor commands"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"results",children:"Results"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Emergent capabilities"}),': Can follow complex instructions ("move the apple to the left of the banana")']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reasoning"}),': "Which object is the heaviest?" \u2192 picks the correct one']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Generalization"}),": 3x better than RT-1 on novel objects"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"implementing-a-simple-vla-model",children:"Implementing a Simple VLA Model"}),"\n",(0,s.jsx)(n.h3,{id:"step-1-data-collection",children:"Step 1: Data Collection"}),"\n",(0,s.jsxs)(n.p,{children:["Collect tuples of ",(0,s.jsx)(n.code,{children:"(image, language, action)"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom sensor_msgs.msg import Image\r\nfrom geometry_msgs.msg import Twist\r\nimport cv2\r\nfrom cv_bridge import CvBridge\r\n\r\nclass DataCollector:\r\n    def __init__(self):\r\n        self.bridge = CvBridge()\r\n        self.data = []\r\n    \r\n    def collect(self, image_msg, cmd_vel_msg, language_command):\r\n        # Convert ROS image to numpy\r\n        image = self.bridge.imgmsg_to_cv2(image_msg, \"rgb8\")\r\n        \r\n        # Extract action\r\n        action = [\r\n            cmd_vel_msg.linear.x,\r\n            cmd_vel_msg.linear.y,\r\n            cmd_vel_msg.angular.z\r\n        ]\r\n        \r\n        # Store\r\n        self.data.append({\r\n            'image': image,\r\n            'language': language_command,\r\n            'action': action\r\n        })\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-model-architecture-pytorch",children:"Step 2: Model Architecture (PyTorch)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nfrom transformers import CLIPModel, CLIPProcessor\r\n\r\nclass SimpleVLA(nn.Module):\r\n    def __init__(self, action_dim=3):\r\n        super().__init__()\r\n        \r\n        # Vision encoder (CLIP)\r\n        self.clip = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")\r\n        self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")\r\n        \r\n        # Freeze CLIP (transfer learning)\r\n        for param in self.clip.parameters():\r\n            param.requires_grad = False\r\n        \r\n        # Action head\r\n        self.action_head = nn.Sequential(\r\n            nn.Linear(512, 256),  # CLIP embedding size = 512\r\n            nn.ReLU(),\r\n            nn.Dropout(0.1),\r\n            nn.Linear(256, action_dim)\r\n        )\r\n    \r\n    def forward(self, images, texts):\r\n        # Get CLIP embeddings\r\n        inputs = self.processor(\r\n            text=texts, \r\n            images=images, \r\n            return_tensors="pt", \r\n            padding=True\r\n        )\r\n        \r\n        outputs = self.clip(**inputs)\r\n        \r\n        # Combine vision and language features\r\n        image_embeds = outputs.image_embeds\r\n        text_embeds = outputs.text_embeds\r\n        combined = image_embeds + text_embeds  # Simple fusion\r\n        \r\n        # Predict actions\r\n        actions = self.action_head(combined)\r\n        return actions\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-training-loop",children:"Step 3: Training Loop"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch.optim as optim\r\n\r\nmodel = SimpleVLA(action_dim=3)\r\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\r\ncriterion = nn.MSELoss()\r\n\r\nfor epoch in range(100):\r\n    for batch in dataloader:\r\n        images = batch['image']\r\n        texts = batch['language']\r\n        actions_gt = batch['action']\r\n        \r\n        # Forward pass\r\n        actions_pred = model(images, texts)\r\n        \r\n        # Loss\r\n        loss = criterion(actions_pred, actions_gt)\r\n        \r\n        # Backward pass\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-4-inference-on-robot",children:"Step 4: Inference on Robot"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import Twist\r\nfrom sensor_msgs.msg import Image\r\n\r\nclass VLAController(Node):\r\n    def __init__(self):\r\n        super().__init__('vla_controller')\r\n        self.model = SimpleVLA()\r\n        self.model.load_state_dict(torch.load('vla_model.pth'))\r\n        self.model.eval()\r\n        \r\n        self.image_sub = self.create_subscription(\r\n            Image, '/camera/image_raw', self.image_callback, 10)\r\n        self.cmd_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n        \r\n        self.current_command = \"move forward\"\r\n    \r\n    def image_callback(self, msg):\r\n        # Convert image\r\n        image = self.bridge.imgmsg_to_cv2(msg, \"rgb8\")\r\n        \r\n        # Predict action\r\n        with torch.no_grad():\r\n            action = self.model([image], [self.current_command])\r\n        \r\n        # Publish\r\n        cmd = Twist()\r\n        cmd.linear.x = float(action[0, 0])\r\n        cmd.linear.y = float(action[0, 1])\r\n        cmd.angular.z = float(action[0, 2])\r\n        self.cmd_pub.publish(cmd)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"open-source-vla-models",children:"Open-Source VLA Models"}),"\n",(0,s.jsx)(n.h3,{id:"openvla-stanford-2024",children:"OpenVLA (Stanford, 2024)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"OpenVLA"})," is an open-source 7B parameter VLA model."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Installation"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install openvla\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from openvla import OpenVLA\r\n\r\nmodel = OpenVLA.from_pretrained("openvla/openvla-7b")\r\n\r\n# Inference\r\naction = model.predict(\r\n    image=camera_image,\r\n    instruction="pick up the red cup"\r\n)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"octo-uc-berkeley-2024",children:"Octo (UC Berkeley, 2024)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Octo"})," is a generalist robot policy trained on 800k robot trajectories."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from octo.model import OctoModel\r\n\r\nmodel = OctoModel.load_pretrained("hf://rail-berkeley/octo-base")\r\n\r\naction = model.sample_actions(\r\n    observation={"image": image},\r\n    task={"language_instruction": "open the drawer"}\r\n)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"challenges-and-limitations",children:"Challenges and Limitations"}),"\n",(0,s.jsx)(n.h3,{id:"1-data-hunger",children:"1. Data Hunger"}),"\n",(0,s.jsxs)(n.p,{children:["VLA models require ",(0,s.jsx)(n.strong,{children:"millions"})," of robot interactions."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simulation"}),": Generate data in Isaac Sim"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Teleoperation"}),": Human demonstrations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Sharing"}),": Open-source datasets (Open X-Embodiment)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-safety",children:"2. Safety"}),"\n",(0,s.jsxs)(n.p,{children:["End-to-end models are ",(0,s.jsx)(n.strong,{children:"black boxes"})," \u2192 hard to guarantee safety."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Residual policies"}),": VLA + classical safety controller"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Constrained action spaces"}),": Limit dangerous actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Human-in-the-loop"}),": Require approval for critical actions"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-computational-cost",children:"3. Computational Cost"}),"\n",(0,s.jsx)(n.p,{children:"Large VLA models (7B+ parameters) are slow on edge devices."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model quantization"}),": INT8, INT4 precision"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Knowledge distillation"}),": Train smaller student model"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hardware acceleration"}),": NVIDIA Jetson Orin, TPUs"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"lab-exercise-vla-for-humanoid-navigation",children:"Lab Exercise: VLA for Humanoid Navigation"}),"\n",(0,s.jsx)(n.h3,{id:"objective",children:"Objective"}),"\n",(0,s.jsx)(n.p,{children:"Train a simple VLA model to navigate based on language commands."}),"\n",(0,s.jsx)(n.h3,{id:"step-1-collect-data",children:"Step 1: Collect Data"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 run data_collector collect_nav_data.py\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Commands"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"move forward"'}),"\n",(0,s.jsx)(n.li,{children:'"turn left"'}),"\n",(0,s.jsx)(n.li,{children:'"turn right"'}),"\n",(0,s.jsx)(n.li,{children:'"stop"'}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"step-2-train-model",children:"Step 2: Train Model"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python train_vla.py --data nav_data.pkl --epochs 50\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-deploy",children:"Step 3: Deploy"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 run vla_controller vla_nav_node\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Test"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 topic pub /vla/command std_msgs/String \"data: 'turn left'\"\n"})}),"\n",(0,s.jsx)(n.h2,{id:"quiz",children:"Quiz"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"What is the main advantage of VLA models over modular pipelines?"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A) Faster computation"}),"\n",(0,s.jsx)(n.li,{children:"B) End-to-end learning reduces error propagation"}),"\n",(0,s.jsx)(n.li,{children:"C) Easier to debug"}),"\n",(0,s.jsx)(n.li,{children:"D) Requires less data"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Answer: B"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Which foundation model is used for visual grounding in VLA?"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A) GPT-4"}),"\n",(0,s.jsx)(n.li,{children:"B) BERT"}),"\n",(0,s.jsx)(n.li,{children:"C) CLIP"}),"\n",(0,s.jsx)(n.li,{children:"D) ResNet"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Answer: C"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"What was RT-2's key innovation over RT-1?"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A) Larger robot arm"}),"\n",(0,s.jsx)(n.li,{children:"B) Pre-training on web-scale vision-language data"}),"\n",(0,s.jsx)(n.li,{children:"C) Faster inference"}),"\n",(0,s.jsx)(n.li,{children:"D) Smaller model size"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Answer: B"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'Why are VLA models considered "black boxes"?'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A) They run on GPUs"}),"\n",(0,s.jsx)(n.li,{children:"B) End-to-end learning makes it hard to interpret decisions"}),"\n",(0,s.jsx)(n.li,{children:"C) They use encryption"}),"\n",(0,s.jsx)(n.li,{children:"D) They don't have source code"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Answer: B"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"In this chapter, we explored Vision-Language-Action (VLA) models, the cutting-edge approach that unifies perception, reasoning, and control in a single neural network. We studied RT-1, RT-2, and open-source models like OpenVLA and Octo. We implemented a simple VLA model using CLIP and deployed it on a simulated humanoid. VLA models represent the future of robotics, enabling generalization and natural language control."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Next Chapter"}),": We'll study control theory and balance for bipedal humanoid robots."]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>t});var i=r(6540);const s={},l=i.createContext(s);function o(e){const n=i.useContext(l);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(l.Provider,{value:n},e.children)}}}]);